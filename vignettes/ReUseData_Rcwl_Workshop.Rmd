---
title: "Use R to Create and Execute Reproducible CWL Workflows for Genomic Research"
author: Qian Liu[qian.liu@roswellpark.org]
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Rcwl_variantCall}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Use R to Create and Execute Reproducible CWL Workflows for Genomic Research

Authors:
	Qian Liu ^[Roswell Park Comprehensive Cancer Center],
    Another Author^[Roswell Park Comprehensive Cancer Center].
    <br/>
Last modified: July 27, 2023.

## Overview

### Pre-requisites

- Basic familiarity with DNA-seq data variant calling 
- Interest of using workflow language 

### Workshop Participation

The workshop format is a 45 minute session consisting of hands-on demos, exercises and Q&A.

### R / Bioconductor packages used
- ReUseData
- RcwlPipelines
- Rcwl


### Description

In this workshop, we will demonstrate how to use `RcwlPipelines` and
`ReUseData` to implement variant calling workflows within R using the
`Mutect2` from GATK. This whole workflow is based on R programming
language and can be deployed in local computer, HPC and cloud
computing platforms, using `docker`, `singularity` or
`udocker`. [FIXME] 

This workshop will be instructor-led live demo with executable code
with real DNA-seq data.  

[DELETE] Along with the topic of your workshop, include
how students can expect to spend their time. For the description may
also include information about what type of workshop it is
(e.g. instructor-led live demo, lab, lecture + lab, etc.). Instructors
are strongly recommended to provide completely worked examples for lab
sessions, and a set of stand-alone notes that can be read and
understood outside of the workshop.

### Pre-requisites

List any workshop prerequisites, for example:

* Basic knowledge of R syntax
* Familiarity with the GenomicRanges class
* Familiarity with xyz vignette (provide link)

List relevant background reading for the workshop, including any
theoretical background you expect students to have.

* List any textbooks, papers, or other reading that students should be
  familiar with. Include direct links where possible.

### Participation

Describe how students will be expected to participate in the workshop.

### _R_ / _Bioconductor_ packages used

```
Rcwl
RcwlPipelines
ReUseData [FIXME: more?]
```
List any _R_ / _Bioconductor_ packages that will be explicitly covered.

### Time outline

An example for a 45-minute workshop:

| Activity                     | Time |
|------------------------------|------|
| Packages                     | 15m  |
| Package Development          | 15m  |
| Contributing to Bioconductor | 5m   |
| Best Practices               | 10m  |

### Workshop goals and objectives

List "big picture" student-centered workshop goals and learning
objectives. Learning goals and objectives are related, but not the
same thing. These goals and objectives will help some people to decide
whether to attend the conference for training purposes, so please make
these as precise and accurate as possible.

*Learning goals* are high-level descriptions of what
participants will learn and be able to do after the workshop is
over. *Learning objectives*, on the other hand, describe in very
specific and measurable terms specific skills or knowledge
attained. The [Bloom's Taxonomy](#bloom) may be a useful framework
for defining and describing your goals and objectives, although there
are others.

### Learning goals

Some examples:

* describe how to...
* identify methods for...
* understand the difference between...

### Learning objectives

* analyze xyz data to produce...
* create xyz plots
* evaluate xyz data for artifacts


## Workshop: Somatic variant calling
## Workshop: downstream: variant annotation/filtering in R

For the somatic variant calling, we will need to prepare the following: 

- Experiment data 
  - In the format of `.bam`, `.bam.bai` files
- ReUsable Genomic data 
  - reference sequence file (`b37` or `hg38`)
  - Panel of Normals (PON) [ref](https://gatk.broadinstitute.org/hc/en-us/articles/360035890631-Panel-of-Normals-PON-)
- Software tool: 
  - Here we use `Mutect2`to Call somatic SNVs and indels via local assembly of
    haplotypes. [ref](https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2)

There will be three ways to do the analysis task. 

1. Command-line interface. We can install the software and write shell scripts. 

2. Workflow language. We can use workflow language (e.g., CWL, WDL,
   nextflow, snakemake) to streamline the workflow of data analysis,
   using containerized tools (no software installation, version
   tracking, etc.)

   - More efficient (especially if multiple tools are included)
   - REPRODUCIBLE!
   - Need to install a workflow runner (e.g., cwltool,
     arvados-cwl-runner) [FIXME: install cwl?]
   - Learning curve. 

3. `Rcwl/RcwlPipelines`. Write/use CWL-based data analysis tools/workflows within
   _R_ programming language 
   
   - All benefits of workflow language
   - Hands-on experience
   - Easy connection with downstream analysis tools in _R/Bioconductor_.
   
   `ReUseData`: Reproducible and reusable genomic data management.
   - Public genomic data resources can be easily managed and reused in
     different projects.

In this workshop, we will demonstrate three _Bioconductor_ packages:
`Rcwl` as an R interface for `CWL`; `RcwlPipelines` with >200
pre-built bioinformatics tools and best practice pipelines in _R_,
that are easily usable and highly customizable; and `ReUseData` for
the management of reusable genomic data.

With these tools, we should be able to conduct reproducible data
analysis using commonly used bioinformatics tools (including
command-line based tools and _R/Bioconductor_ packages) and validated,
best practice workflows (based on workflow languages such as CWL)
within a unified _R_ programming environment.

### Prepare the software tools in _R_ using `RcwlPipelines` 

`RcwlPipelines` includes more than 200 pre-built, commonly used
bioinformatics tools, such as `BWA` for DNA read alignment, `STAR` for
RNA read quantification, `Mutect2` for somatic variant calling. Here
we use `Mutect2` to demonstrate the use of these tools to run
CWL-based data analysis workflow within _R_.

Below we will show 3 core functions: `cwlUpdate`, `cwlSearch` and
`cwlLoad` to update, search and load the needed tool or pipeline in
_R_.

#### Load tool or pipeline

The `cwlUpdate` function syncs the current `Rcwl` tool recipes in the
local cache. It returns a `cwlHub` object that contains the most
updated `Rcwl` recipes. User need to call this function for first-time
use or if they want to use a newly added tool/pipeline from
`RcwlPipelines`.

```{r}
library(RcwlPipelines)
library(Rcwl)
cwlUpdate()
```

Then the existing tool recipes can be queried using with `cwlSearch`
with multiple keywords. Then it's ready to be loaded into _R_ with
`cwlLoad`.

Multiple tool/recipes are available with `Mutect2`. We first show the
pipeline which includes multiple tools for the tasks of variant
calling, QC, and filtering [FIXME].

```{r}
cwlSearch("mutect2")
mutect2pl <- cwlLoad("pl_Mutect2PL")
plotCWL(mutect2pl)
```

In this workshop, we will use the simple tool of `tl_Mutect2` for
variant calling only. We can customize the tools by using a smaller
docker image for demo purposes. 

```{r}
mutect2 <- cwlLoad("tl_Mutect2")
## change to smaller docker image
ds <- searchContainer("gatk4")
req1 <- requireDocker(ds[1, "container"])
requirements(mutect2)[[1]] <- req1
requirements(mutect2)
```

#### Check data inputs 

We can use the `Rcwl::inputs` function to check the required input
parameters for the tool. For `Mutect2`, multiple parameters are
defined to map to the tool arguments. Major inputs would be the BAM
files containing reads (`tbam` and `nbam` for tumor and normal
separately), the reference sequence file (`Ref`), and the `pon` for
panel of normals VCF files. 

Some input parameters come with default values (such as `f1r2`, which
can be changed easily. More details about the tool arguments can be
found
[here](https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2).

```{r}
inputs(mutect2)
```

We will assign values to the input parameters before evaluation. 

For the experiment data, we have prepared a small DNA-seq dataset for
demo purposes. 

For the public genomic data resources that are needed to run `mutect2`
for somatic variant calling: the reference sequence file (`Ref`) and
PON VCF file (`pon`), we can assign the file path directly to the
corresponding parameters if you already have them, and evaluate the
data recipe now with `runCWL` function.

However, as these genomic data files can be repeatedly used in many
different projects (e.g., DNA-seq data), we propose an
_R/Bioconductor_ package `ReUseData` to manage these files so that
they can be easily tracked for tool/data provenance to safely
reproduce and reuse.

```{r}
mutect2$tbam <- system.file("extdata/tumor.bam", package="RcwlWorkshop")
mutect2$nbam <- system.file("extdata/normal.bam", package="RcwlWorkshop")
mutect2$normal <- "NA12892"
mutect2$out <- "somatic.vcf"
## prepare capture region
write.table(rbind(c(21, 10400000, 10500000)), file.path(workdir, "region.bed"),
            row.names=FALSE, col.names = FALSE, quote=FALSE, sep="\t")
mutect2$interval <- file.path(workdir, "region.bed")
inputs(mutect2)
## mutect2$Ref <-
## mutect2$pon <-
## runCWL(mutect2, outdir = file.path(workdir, "output"), docker = "udocker", showLog = TRUE)
```

### ReUseData: Reusable and Reproducible Management of Genomic Data Resources 

`ReUseData` facilitates transformation of shell scripts for data
preprocessing (e.g., downloading, indexing) into workflow-based data
recipes. Evaluation of data recipes generate curated data files in
their generic formats (e.g., VCF, bed) with automatically generated
meta files, this will help track data and tool provenance for
subsequent data reuse.


There are some pre-built data recipes in `ReUseData` that are ready to
be queried and used. Basically, we run the `recipeUpdate` function to
synchronize the existing and newly added recipes (by default the
recipes included in [`ReUseData` package](https://github.com/rworkflow/ReUseDataRecipe), 
can also be a user-specific GitHub repository with similar settings).

The prebuilt data recipe scripts are included in the package, and are
physically residing in a dedicated [GitHub
repository](https://github.com/rworkflow/ReUseDataRecipe). These can
serve as template scripts for recipe construction in different
situations. The most common case is that a data recipe can manage
multiple data resources with different input parameters (species,
versions, etc.). For example, the `gencode_transcripts` recipe
download from GENCODE, unzip and index the transcript fasta file for
human or mouse with different versions. A simple data downloading
(using `wget`) for a specific file can be written as a data recipe
without any input parameter. For example, The pre-built recipes of
`gcp_gatk_mutect2_b37` and `gcp_gatk_mutect2_hg38` are for the
downloading of genomic data resources from the GATK resource bundle on
[Google Cloud
Bucket](https://console.cloud.google.com/storage/browser/gatk-best-practices/somatic-b37)
based on reference build of `b37` and `hg38` separately. See more
details
[here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890811-Resource-bundle).

In this workshop, we will use the `gcp_gatk_mutect2_b37` data recipe
to download necessary genomic data files for the somatic variant
calling.

#### Data recipes 

We will 4 core functions to update (`recipeUpdate`), search
(`recipeSearch`), load (`recipeLoad`) the data recipe, assign values
to the input parameters (`filename` and `idx`), and then evaluate the
recipe (`getData`) to download the data to your local machine with
automatic tracking and user-specified notes/keywords.

The `getData` function evaluates the data recipe and generate the
desired data output in a user-specified `outdir`. Arbitrary `notes`
can be added for the dataset for easy query later.

Internally, the data recipe is submitted as a cwl task evaluting the
shell script for data downloading/processing.

```{r}
library(ReUseData)
workdir <- "/tmp/Bioc2023_rcwl"
dir.create(workdir)
```

```{r}
recipeUpdate()
recipeSearch("mutect2")
rcp <- recipeLoad("gcp_gatk_mutect2_b37")

## get panel of normal vcf
rcp$filename <- "Mutect2-exome-panel.vcf"
rcp$idx <- "idx"
getData(rcp, outdir = file.path(workdir, "shareData"),
        notes = c("mutect2", "panel of normal"),
        showLog = TRUE)

## check output
list.files(file.path(workdir, "shareData"), pattern= c("panel", "normal"))
```

The output directory includes the downloaded genomic file dataset, as
well as some automatically generated meta files (ending with `md5`,
`cwl`, `yml` and `sh`) by `getData`, where the the meta information
for data recipe are recorded for subsequent reuse. More details about
these meta files, please see the `For developers` section below.

Similarly, we will download the reference genome files using the same
data recipe:

```{r}
## Reference genome files
rcp$filename <- "Homo_sapiens_assembly19.fasta"
rcp$idx = "fai"
getData(rcp, outdir = file.path(workdir, "shareData"),
        notes = c("human", "reference genome", "hg19", "b37"),
        showLog = TRUE)

rcp$filename <- "Homo_sapiens_assembly19.dict"
rcp$idx <- ""
getData(rcp, outdir = file.path(workdir, "shareData"),
        notes = c("human", "reference genome dict", "hg19", "b37"),
        showLog = TRUE)
## check output
list.files(file.path(workdir, "shareData"), pattern = c("assembly19"))
```

Now we have all the data needed. Here we show some utility functions
to manage, query, and use the data.

#### Data management

There are 2 core functions to manage the data locally for easy query
and use.

Then we use `dataUpdate` to cache the data in specified data directory
`dir`. Then the data can be easily tracked and queried using
`dataSearch` function. Meta information about the data, such as file
paths, can be easily retrieved for later use.

- `dataUpdate`: Cache the data in specified data directory `dir`
  (works recursively). Need to call for first-time use or if any new
  datasets are generated locally with `getData`.
- `dataSearch`: Query of cached datasets with (multiple) keyword(s)
  (can be keywords from the file name or the user-specified `notes`.

```{r}
dataUpdate(dir = workdir, keepTags=FALSE, cleanup = TRUE)
rs1 <- dataSearch("mutect2")
rs2 <- dataSearch("reference")
```

Besides the `notes` added in `getData`, users can further tag the
data. For example, if a set of datasets can be used as inputs for a
specific software tool, we'll tag these datasets with the software
name, so that they can be retrieved easily with that keyword in
`dataSearch`.

Here we'll tag the reference genome data with `mutect2`, and use some
utility functions to retrieve specific information about the
data. Data path can be retrieved using `dataPaths` function, which
will be passed directly to the `Rcwl` tool recipe for reproducible
data analysis in _R_.
 
```{r}
dataTags(rs2[2]) <- "mutect2"
rs <- dataSearch("mutect2")
rs
ref <- dataPaths(rs)[1]
pon <- dataPaths(rs[2])
## try: dataNames, dataYml, dataNotes, dataParams, etc.
```

### Run reproducible data analysis in _R_

Coming back to the `RcwlPipelines` tool recipe, here we will assign
values to the tool for the reusable genomic data resources that are
managed by `ReUseData`, and evaluate the tool (internally submit the
CWL task) using `runCWL` for reproducible data analysis in _R_.

```{r}
## check inputs
inputs(mutect2)
## assign inputs of reusable genomic data
mutect2$Ref <- ref
mutect2$pon <- pon
inputs(mutect2)
runCWL(mutect2, outdir = file.path(workdir, "output"),
       docker = "udocker", showLog = TRUE)
```

The results should be successfully generated in the user-specified
output directory. The `somatic.vcf` contains all the somatic variants
that are called from the input bam files using the bioinformatics
software tool `Mutect2`.

```{r}
## checkout results
list.files(file.path(workdir, "output"))
```

### Downstream data analysis using _R/Bioconductor_ packages 

[FIXME]

```{r}
library(VariantAnnotation)
fl <- file.path(workdir, "output/somatic.vcf")
## readVCF(fl, )

```

### For developers

#### Data annotation files

By evaluating a `ReUseData` data recipe with `getData`, some meta
files for the data will be automatically generated for data and tool
provenance.


By default, the meta file names for these meta files are paste of
recipe name, and input parameter values separated by `_`, which can be
changed by the `prefix` argument in `getData` function if needed.

```{r}
list.files(file.path(workdir, "shareData"), pattern = "cp_gatk_mutect2_b37")  ## meta files
```

- `[recipeName_params].cwl`: Rcwl object defined in `Rcwl` for
  tool or data recipe.
- `[recipeName_params].yml`: File containing values for input
parameters for both tool and data recipes through recipe evaluation
functions (e.g., `runCWL`, `getData`). For data recipes, it includes
additional meta information for the output files, notes, and date, etc
for data tracking purposes, which are added by `getData` function.
- `[recipeName_params].sh`: The command lines in a shell script. 
- `[recipeName_params].md5`: unique identifier for each dataset.


#### Create data recipe

Here we use a simple example to show how to create a data recipe. This
recipe will do a simple task of just downloading specific files from
fixed web source without any input parameter.

```{r}
## library(ReUseData)
```

The `recipeMake` function will wrap the command line script for data
downloading/processings into an executable data recipe in R. We will
specify the inputs and outputs of the data recipe and use `outputGlob`
to specify the output pattern (for internal check). 

```{r}
script <- '
wget https://github.com/hubentu/somatic-snv-test-data/raw/master/tumor.bam
wget https://github.com/hubentu/somatic-snv-test-data/raw/master/tumor.bam.bai
wget https://github.com/hubentu/somatic-snv-test-data/raw/master/normal.bam
wget https://github.com/hubentu/somatic-snv-test-data/raw/master/normal.bam.bai
'
rcp_expdata <- recipeMake(shscript = script,
                          outputID = "bams",
                          outputGlob = "*.bam*")
```

We need to assign values to the input parameters if they exist (data
inputs for the shell script). Then the data recipe is ready for the
evaluation using `getData` function (see the `ReUseData` section [FIXME]
above).

#### Data use in other way

[FIXME] toList()

```{r}
dataUpdate(dir = workdir)
ds1 <- dataSearch(c("bam"))
dataPaths(ds1)
```

